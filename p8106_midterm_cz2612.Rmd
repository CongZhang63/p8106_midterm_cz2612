---
title: "Midterm Project"
author: "Cong Zhang"
date: 2021-03-21
output: html_document
---

This is my solution to Midterm Project.

```{r include = FALSE, message = FALSE}
library(tidyverse)
library(visdat)
library(caret)
library(glmnet)
library(pls)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(ISLR)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

### Prepare and clean the data

```{r message = FALSE}
housing <-
  read_csv("./data/housing.csv") %>% 
  janitor::clean_names() %>% 
  mutate(chas = as.factor(chas))

vis_miss(housing)

set.seed(1)
row_train <- createDataPartition(y = housing$crim, p = 0.75, list = FALSE)

# training data
x_train <- model.matrix(crim ~ ., housing)[row_train,-1]
y_train <- housing$crim[row_train]
data_train <- subset(housing[row_train,])

# test data
x_test <- model.matrix(crim ~ ., housing)[-row_train,-1]
y_test <- housing$crim[-row_train]
data_test <- subset(housing[-row_train,])
```


### Perform exploratory data analysis/visualization

```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(0.2, 0.4, 0.2, 0.5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(0.8, 0.1, 0.1, 0.1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(0.0, 0.2, 0.6, 0.2)

trellis.par.set(theme1)

featurePlot(x_train, y_train,
            plot = "scatter",
            labels = c("","Y"),
            type = c("p"),
            layout = c(4,4))
```


### Train 7 models using repeated 10-fold cross-validation

Cross validation methods
```{r}
ctrl1 <- trainControl(method = "repeatedcv", repeats = 5)
```


Linear Regression
```{r lm}
set.seed(1) 
lm.fit <- train(crim ~ .,
                data = data_train,
                method = "lm",
                trControl = ctrl1)

summary(lm.fit)

lm.pred <- predict(lm.fit, newdata = data_test)
mean((lm.pred - data_test$crim)^2)
```
From the results, we could see that rad and medv are significant predictor variables at 0.1% level; dis is a significant predictor variable at 1% level, zn is a significant predictor variable at 5% level.


Ridge Regression
```{r ridge}
set.seed(1)
ridge.fit <- train(crim ~ .,
                   data = data_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(5, -5, length = 100))),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune

coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)

ridge.pred <- predict(ridge.fit, newdata = data_test)
mean((ridge.pred - data_test$crim)^2)
```


Lasso Model
```{r lasso}
set.seed(1)
lasso.fit <- train(crim ~ .,
                   data = data_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(5, -5, length = 100))),
                   trControl = ctrl1)
    
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

lasso.pred <- predict(lasso.fit, newdata = data_test)
mean((lasso.pred - data_test$crim)^2)
```
From the result, we could see that the number of non-zero coefficient estimates (excluding the intercept) of the Lasso Model is `r sum(coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda) != 0) - 1`


Elastic Net Model
```{r elastic net}
set.seed(1)
enet.fit <- train(crim ~ .,
                  data = data_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 11), 
                                         lambda = exp(seq(5, -5, length = 50))),
                  trControl = ctrl1)

plot(enet.fit)

enet.fit$bestTune

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)

enet.pred <- predict(enet.fit, newdata = data_test)
mean((enet.pred - data_test$crim)^2)
```
From the result, we could see that the number of non-zero coefficient estimates (excluding the intercept) of the Elastic Net Model is `r sum(coef(enet.fit$finalModel, enet.fit$bestTune$lambda) != 0) - 1`


Principle Components Regression
```{r pcr}
set.seed(1)
pcr.fit <- train(crim ~ .,
                 data = data_train,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:(ncol(data_train) - 1)), 
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

pcr.fit$bestTune

validationplot(pcr.fit$finalModel, val.type = "MSEP") 

pcr.pred <- predict(pcr.fit, newdata = data_test)
mean((pcr.pred - data_test$crim)^2)
```


Generalized Additive Model
```{r gam}
set.seed(1)
gam.fit <- train(crim ~ .,
                 data = data_train,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp",select = c(TRUE, FALSE)),
                 trControl = ctrl1)
gam.fit$bestTune

gam.fit$results

gam.fit$finalModel

plot(gam.fit$finalModel)

gam.pred <- predict(gam.fit, newdata = data_test)
mean((gam.pred - data_test$crim)^2)
```
According to the results of GAM model, the degrees of freedom of the s functions corresponding to variable zn, indus, and age are 0. Therefore, these variables do not enter the model, and we discard them when doing variable selection.


MARS Model
```{r mars}
mars_grid <- expand.grid(degree = 1:3, nprune = 2:30)

set.seed(1)
mars.fit <- train(crim ~ .,
                  data = data_train,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune

mars.fit$finalModel

coef(mars.fit$finalModel)

mars.pred <- predict(mars.fit, newdata = data_test)
mean((mars.pred - data_test$crim)^2)
```


### Compare performance of the 7 models

```{r}
resamp <- resamples(list(lm = lm.fit,
                         ridge = ridge.fit,
                         lasso = lasso.fit,
                         enet = enet.fit,
                         pcr = pcr.fit,
                         gam = gam.fit,
                         mars = mars.fit))

bwplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "Rsquared")
summary(resamp)
```

